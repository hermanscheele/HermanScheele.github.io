FYS-STK3155 Pro ject 1
Herman Scheele, Theo dor Jaarvik, Elaha Ahmadi
Septemb er 2024
Abstract
Linear regression is a fundamental statistical metho d widely employed
for mo deling the relationship b etween a dep endent variable and one or
more indep endent variables. This pap er investigates the use of Ordinary
Least Squares (OLS), Ridge, and Lasso regression techniques for estimat-
ing parameters in a mo del tting the Franke Function and applying them
to real-world terrain data. We examine the statistical prop erties of our
data and optimal parameters, discussing the bias-variance tradeo and
employing resampling metho ds such as cross-validation and b o otstrap to
assess mo del p erformance. Our main ndings indicate that b oth OLS and
Ridge regression p erformed eectively on datasets of mo derate complexity,
with Ridge regression providing b etter generalization due to its regular-
ization. Conversely, Lasso regression tended to undert the data, likely
due to excessive p enalization of imp ortant features, particularly evident
in the real-world terrain data where it failed to capture crucial details.
These results highlight the imp ortance of selecting appropriate regression
metho ds and the role of regularization in handling overtting and f eature
selection in datasets.
1 Intro duction
Machine learning has b ecome an essential to ol for solving various c omple x prob-
lems in b oth academic and industrial elds. Regression analysis, in particular,
plays a crucial role in mo deling and predicting relationships b etween variables.
A key challenge in regression is selec ting appropriate mo dels that balance com-
plexity and computational cost while eectively capturing the underlying pat-
terns in the data.
In this pro ject, we fo cus on regression analysis using metho ds such as Ordi-
nary Leas t Squares (OLS), Ridge, and Lasso regression, as well as resampling
techniques like b o otstrap and cross-validation. Our goal is to c ompare the ef-
fectiveness of these dierent regression mo dels and understand how resampling
techniques can help in evaluating mo del p erformance, particularly in terms of
the bias-variance tradeo and overtting prevention.
1
We apply these mo dels to b oth synthetic data generated by the Franke Func-
tion and real-world terrain data to assess their p e rformance in dierent contexts.
The Franke Function provides a controlled environment with known prop erties
and adjustable noise, allowing us to evaluate how well each regression metho d
can t complex, nonlinear surfaces. The real-world terrain data intro duces addi-
tional complexity and noise, testing the m o dels ability to generalize to practical
applications.
The s tructure of this rep ort is as follows: We b egin by applying OLS to the
Franke Function, establishing a baseline for p erformance. We then intro duc e
regularization techniques with Ridge and Lasso regression to address p otential
ove rtting issues. Then, we will derive some of the statistical interpretations
of linear regression. Next, we conduct a bias-variance trade-o analysis using
resampling metho ds to evaluate mo del generalization. Finally, we apply the
develop ed mo dels to analyze re al digital terrain data, discussing the practical
implications of our ndings.
Through this comprehensive study, we aim to provide insights into the advan-
tages and limitations of each regression metho d, oering guidance on selec ting
appropriate technique s for dierent typ es of datasets and highlighting the im-
p ortance of regularization in handling ove rtting and feature selection.
2 Metho ds
2.1 The Franke Function
In this s tudy, we employ the Franke Function as a test case to evaluate the
p erformance of various linear regression techniques, including Ordinary Least
Squares (OLS), Ridge Regression, and Lasso Regression. The Franke Function
is a well-known b e nchmark function in numerical analysis, interp olation and
tting algorithms, particularly useful for testing regression algorithms due to
its nonlinear and complex structure.
The Franke function, which is a weighted sum of four exp onentials reads as
follows.
f
(
x; y
) =
3
4
exp


(9
x

2)
2
4

(9
y

2)
2
4

+
3
4
exp


(9
x
+ 1)
2
49

(9
y
+ 1)
10

+
1
2
exp


(9
x

7)
2
4

(9
y

3)
2
4


1
5
exp


(9
x

4)
2

(9
y

7)
2

:
Our input functions will b e dened for
x; y
2
[0
;
1]. In a sense, our data are thus
scaled to a particular domain for the input values. We will also add sto chastic
noise to our function data using the normal distribution

Ë˜
N
(
 Ë™
2
) to simu-
late more realistic data. For the co de implementation for our data we will b e
2
using co de gathered from the FYS-STK3155 jupyter-noteb o ok. [2].
1
# Franke function definition
2
def
 FrankeFunction
(x, y):
3
term1
 =
 0.75
 *
 np
.
exp(
-
(
0.25
*
(
9
*
x
 -
 2
)
**
2
)
 -
 0.25
*
((
9
*
y
 -
 2
)
**
2
))
4
term2
 =
 0.75
 *
 np
.
exp(
-
((
9
*
x
 +
 1
)
**
2
)
 /
 49.0
 -
 0.1
*
(
9
*
y
 +
 1
))
5
term3
 =
 0.5
 *
 np
.
exp(
-
(
9
*
x
 -
 7
)
**
2
 /
 4.0
 -
 0.25
*
((
9
*
y
 -
 3
)
**
2
))
6
term4
 =
 -
0.2
 *
 np
.
exp(
-
(
9
*
x
 -
 4
)
**
2
 -
 (
9
*
y
 -
 7
)
**
2
)
7
return
 term1
 +
 term2
 +
 term3
 +
 term4
8
9
np
.
random
.
seed(
2018
)
10
# Define the fixed step size and generate grid points
11
step_size
 =
 0.02
# Adjust this value for different step sizes
12
x_values
 =
 np
.
arange(
0
,
 1
, step_size)
13
y_values
 =
 np
.
arange(
0
,
 1
, step_size)
14
x_grid, y_grid
 =
 np
.
meshgrid(x_values, y_values)
15
x
 =
 x_grid
.
ravel()
16
y
 =
 y_grid
.
ravel()
17
n
 =
 len
(x)
18
19
# z-values for the franke function
20
z
 =
 FrankeFunction(x, y)
 +
 np
.
random
.
normal(
0
,
 1
, x
.
shape)
# Adding Gaussian noise
21
2.2 Linear regression and its applications
To t the generated data, we employ three linear re gre ssion techniques: Ordi-
nary Least Squares (OLS), Ridge Regression, and Lasso Regression. We will
evaluate each mo del using the Mean Squared Error and the R2 sc ore which
reads as follows.
MSE =
1
n
n
X
i
=1
(
y
i

^
y
i
)
2
(1)
R
2
= 1

P
n
i
=1
(
y
i

^
y
i
)
2
P
n
i
=1
(
y
i


y
)
2
(2)
In our study, we apply regression techniques with several key ob jectives.
First, transforming the input variable s into p olynomial features allows the lin-
ear mo dels to c apture and approximate the nonlinear surface of the Franke
Function. By comparing the p erformance of OLS, Ridge, and Lasso, we can
evaluate how regulariz ation aec ts mo del accuracy and complexity. Regular-
ization techniques , particularly in Ridge and Lasso, are essential for preventing
ove rtting by striking a balance b etwe en tting the training data and maintain-
ing predictive p erformance on uns een data. Finally, Lasso Regression's inherent
3
feature selection enhances mo del interpretability by identifying the most signif-
icant feature s, providing deep er insights into the underlying data.
2.3 Ordinary Least Squares
Ordinary Least Squares (OLS) regression is a fundamental statistical metho d
used to estimate the re lations hip b etween a dep endent variable and one or more
indep endent variables. OLS aims to nd the co e cient vector

that minimizes
the sum of squared residuals b etween the observed values
z
and the predicted
values ^
z
. The estimated co ecient vector
^

is obtained by minimizing the sum
of squared residuals:
^

= arg min

k
y

X 
k
2
2
The closed-form solution for the Ordinary Least Squares (OLS) estimator
^

is then given by the normal equation:
^

= (
X
T
X
)

1
X
T
z
(3)
Where X is our design matrix and z is a vector consisting of our z values. When
dealing with multidimensional surfaces like the Franke function we encounter
more than one indep endent variable. In our case we have two, x and y. This
means that we need a x and y dep endence on the form [
x; y ; x
2
; y
2
; xy ; :::
] to
generate a p olynomial t to the data. Our de sign matrix will thus b e on the
form
X
=
0
B
B
B
B
B
@
1
x
1
y
1
x
2
1
y
2
1
x
1
y
1
:::
1
x
2
y
2
x
2
2
y
2
2
x
2
y
2
:::
1
x
3
y
3
x
2
3
y
2
3
x
3
y
3
:::
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1
x
n
y
n
x
2
n
y
2
n
x
n
y
n
:::
1
C
C
C
C
C
A
After nding the co ecient ve ctor
^

, we calculate our predictive value s
~
z
=
X
^

(4)
This is our predictive values that can b e expressed as a linear combination:
~
z
=

0
+

1
x
1
+

2
y
1
+

3
x
2
1
+

4
y
2
1
+

5
x
1
y
1
+
  
4
In this OLS analysis we will b e using p olynomials in
x
and
y
up to the fth
order. Below is the co de implementation of a design matrix and the calculation
of the co e cient vector and the predictive values.
1
def
 create_design_matrix
(x, y, degree):
2
X
 =
 np
.
ones((
len
(x),
 1
))
# Column of ones for the intercept
3
for
 i
 in
 range
(
1
, degree
 +
 1
):
4
for
 j
 in
 range
(i
 +
 1
):
5
X
 =
 np
.
column_stack((X, (x
**
(i
-
j))
 *
 (y
**
j)))
6
return
 X
7
8
# Optimal values for beta calculated with X_train and Z_train
9
beta
 =
 np
.
linalg
.
pinv(X_train_scaled
.
T
 @
 X_train_scaled)
 @
X_train_scaled
.
T
 @
 z_train
,
!
10
betaValues
.
append(beta)
11
12
# predictive model trained and tested on training data
13
model_train
 =
 X_train_scaled
 @
 beta
14
15
# predictive model trained on training data but tested on testing
data
,
!
16
model_test
 =
 X_test_scaled
 @
 beta
2.3.1 Data Splitting: Training and Test sets
To evaluate the p erformance of our OLS regression mo del, we split the dataset
into training and testing subsets. This approach allows us to assess how well
the mo del trained on one p ortion of the data p erforms on unseen data, which is
crucial for detecting overtting and ensuring that the mo del can generalize to
new inputs . To implement this we utilized the
tr ain
test
spl it
function from the
scikit-learn [5] library to randomly partition the data. The dataset was divide d
into 70% for training and 30% for testing, a common split ratio that provides
a balanc e b etween training the mo del eectively and having sucient data to
evaluate its p erformance.
2.3.2 Scaling
Feature scaling is a prepro cessing technique used to standardize the range of
indep endent variables or features in the data. In the context of p olynomial
regression, esp ecially with higher-degree terms, scaling b ecomes imp ortant due
to the p otential for large di erences in the magnitude of feature values . We
applied feature scaling to the design matrix using the StandardScaler class from
scikit-learn [5]. This scaler standardize s features by removing the m ean and
5
scaling to a variance of 1, resulting in a dataset where each feature has a mean
of zero and a standard deviation of 1. This also means that our intercept column
b ecomes a zero vector and we can disregard

0
:
2.3.3 Why Scale the data?
Firstly, scaling helps pre vent numerical issues that can aris e during matrix com-
putations, such as inversion, esp ecially when dealing with p olynomial terms of
varying magnitudes. Secondly, features with larger sc ales can negatively impact
the mo del. Scaling ensures that each feature contributes equally to the mo del's
learning pro cess. Finally, for optimization algorithms, scaled data can lead to
faster running tim es to nd the optimal value s.
2.4 Ridge Regression
Ridge regression is an extension of the OLS metho d that incorp orates a reg-
ularization term to mitigate the eects of overtting, es p ecially in situations
where the des ign matrix
X
contains multicollinear or highly correlated features.
Ridge regression adds a p enalty term to the OLS cost function. This p enalty
term e ectively shrinks the magnitude of the co ecients, res ulting in a m ore
stable and generalizable mo del.
The Ridge regression cost function is dened as:
J
(

) =
1
2
N
N
X
i
=1
(
y
i

^
y
i
)
2
+

p
X
j
=1

2
j
Here,

is a hyp erparameter that controls the strength of regularization. A
larger

increases the p enalty on large co ecients, while a smaller

reduces the
impact of the regularization term, approaching the OLS solution as

!
0.
The close d- form solution for the Ridge regression estimator is given by:
^

ridge
= (
X
T
X
+
I
)

1
X
T
y
Where
I
is the identity matrix, and

is the regularization parameter.
2.4.1 Implementation of Ridge Regression
In our imple mentation, we used the Franke Function to generate the dataset,
adding a small amount of noise to simulate real-world data. The p olynomial
design matrix was created with terms up to the fth degree in
x
and
y
, ensuring
that the mo del could capture the nonlinear surface of the Fra nke function.
6
The following steps outline our Ridge regression pro cess:
1. Data Preparation and Feature Scaling:
As with OLS, the design ma-
trix was standardized using the `StandardScaler` from the scikit-learn library to
ensure that each feature contributed equally to the mo del. Scaling the features
is particularly imp ortant in Ridge regre ssion b ecause it prevents features with
larger magnitudes from dominating the re gularization term.
2. Ri dge Regression Estimator:
The Ridge regression estimator was im-
plemented using the following function, where the regularization parameter

was varied across multiple values to analyze its impact on the mo del p erfor-
mance.
1
def
 RidgeRegression
(X_train, z_train, X_test, lmbda):
2
I
 =
 np
.
eye(X_train
.
shape[
1
])
# Identity matrix
3
beta_ridge
 =
 np
.
linalg
.
pinv(X_train
.
T
 @
 X_train
 +
 lmbda
 *
 I)
@
 X_train
.
T
 @
 z_train
,
!
4
z_pred
 =
 X_test
 @
 beta_ridge
5
return
 z_pred, beta_ridge
3. Mo del Evaluation:
We evaluated the Ridge regression mo dels using the
MSE and the R2 score acros s dierent values of

, ranging from 0.01 to 100. This
allowe d us to explore the tradeo b etween bias and variance in Ridge regression.
As

increases, the mo del b ecomes less se ns itive to the training data, reducing
variance but p otentially increasing bias.
4. Validation and Mo del Selection:
To ensure the validity of our Ridge
regression mo del, we compared the results at

= 0
:
01 to the OLS solution,
observing that the results converged as exp ected. Additionally, cross-validation
could b e implemented to further asses s the generalization p erformance of the
mo del and ne-tune the regularization parameter

.
2.5 Lasso Regression
Lasso regression, like OLS regression, aims to minimize the dierence b etween
predicted and true values. However, it intro duces an additional L1 regular-
ization term in its c ost function, which p enalizes large co ecients. This reg-
ularization can shrink some co e cients to exactly zero, eectively eliminating
irrelevant features. This makes Lasso particularly useful for datasets with many
features, helping in b oth feature selection and reducing overtting.
7
Lasso c ost loss function:
J
(

) =
1
2
N
N
X
i
=1
(
y
i

^
y
i
)
2
+

p
X
j
=1
j

j
j
This pro je ct imple ments [5] Scikit-learn's lasso regression, where the param-
eters of the function are changed to t the dataset. Below is an example of the
co de implementation:
[5] Scikit-learn: Lasso regression
The max iter parameter de nes the maximum numb er of iterations allowed
for the optimization algorithm to converge. The alpha parameter, equivalent
to

in the cost function, controls the strength of the regularization. A higher
alpha value increases the regulariz ation p e nalty, shrinking the co ecients of les s
imp ortant features toward zero, eectively p erforming feature selection. When
alpha is set to 0, the regularization term is remove d, and the mo del b ehaves like
standard OLS regres sion. The visualization b elow shows the interconnection
b etween increasing values of alpha and the

co ecients.
8
Figure 1: Co ecient shrinking for dierent values of lamb da
2.5.1 Impact of

on

Co ecients in Ridge Regression
In Ridge regression, the co ecients

are estimated by minimizing:
^

r idg e
= arg min

jj
y

X

jj
2
+

jj

jj
2

:
As

increases, the regularization term

jj

jj
2
p enalizes large co ecients,
shrinking

towards zero. For

= 0, Ridge regression reduces to OLS, but as

! 1
,

tends to zero, leading to a simpler mo del with higher bias but lower
variance. Thus,

balances mo del complexity by controlling the magnitude of

and mitigating overtting.
2.6 Statistical interpretation of Linear Regression
The analysis b egins with the linear mo del assumption, where the relationship
b etween the observed data
y
and the true underlying function
f
(
x
) is given by:
y
=
f
(
x
) +

Here,

Ë˜
N
(0
; Ë™
2
) represents normally distribute d noise with mean 0 and
variance
Ë™
2
. T he function
f
(
x
) is approximated using a linear mo del ^
y
=
X 
,
where
X
is the design matrix,

represents the vector of parameters, and ^
y
is
9
the pre dicte d value. The parameters

are estimated using the OLS metho d by
minimizing the resudual sumof squares:
^

= (
X
T
X
)

1
X
T
y
2.6.1 Exp ectation Value of
^
y
i
The predicted resp onse ^
y
i
is given by:
^
y
i
=
x
>
i
^

The exp ectation of ^
y
i
is:
E
[ ^
y
i
] =
E
[
x
>
i
^

] =
x
>
i
E
[
^

]
Since
E
[
^

] =

, it follows that:
E
[ ^
y
i
] =
x
>
i

2.6.2 Variance of
^
y
i
We assume the mo del
y
i
=
x
>
i

+

i
, where

i
Ë˜
N
(0
; Ë™
2
).
Since
x
>
i

is deterministic and

i
is normally distributed with variance
Ë™
2
,
we have:
Var(
y
i
) = Var(
x
>
i

+

i
) = Var(

i
) =
Ë™
2
2.6.3 Exp ectation value of
^

To compute the exp ectation and variance of
^

, substitution of
y
=
X 
+

into
the OLS estimator is p erformed, and the prop erties of exp ectation and variance
are applied. The exp ectation
E
(
^

) and variance
V ar
(
^

) are derived as follows:
We start with the ordinary least squares (OLS) estimator for

:
^

= (
X
>
X
)

1
X
>
y
Assuming the mo del is
y
=
X 
+

, where

is the error term with
E
[

] = 0,
we substitute
y
into the expression for
^

:
10
^

= (
X
>
X
)

1
X
>
(
X 
+

)
Expanding this express ion:
^

= (
X
>
X
)

1
(
X
>
X 
+
X
>

)
Since (
X
>
X
)

1
X
>
X
=
I
, we simplify to:
^

=

+ (
X
>
X
)

1
X
>

Now, take the exp ectation of
^

:
E
[
^

] =
E


+ (
X
>
X
)

1
X
>


Since

is constant, we ge t:
E
[
^

] =

+
E

(
X
>
X
)

1
X
>


Given that
E
[

] = 0, the second term vanishes:
E
[
^

] =

2.6.4 Variance of
^

We start with the OLS estimator for
^

:
^

= (
X
>
X
)

1
X
>
y
Substituting the mo del
y
=
X 
+

, we have:
^

= (
X
>
X
)

1
X
>
(
X 
+

)
Expanding this:
^

=

+ (
X
>
X
)

1
X
>

To calculate the variance of
^

, we consider only the second term since

is
constant:
Var(
^

) = Var

(
X
>
X
)

1
X
>


Since (
X
>
X
)

1
X
>
is constant, we c an factor it out of the variance:
Var(
^

) = (
X
>
X
)

1
X
>
Var(

)
X
(
X
>
X
)

1
By assumption,

Ë˜
N
(0
; Ë™
2
I
), so the variance of

is:
Var(

) =
Ë™
2
I
11
Substituting this into the variance expression for
^

:
Var(
^

) = (
X
>
X
)

1
X
>
(
Ë™
2
I
)
X
(
X
>
X
)

1
Since
X
>
X
(
X
>
X
)

1
=
I
, the expression simplies to:
Var(
^

) =
Ë™
2
(
X
>
X
)

1
2.6.5 For Ridge
In Ridge regression, the OLS estimation is mo died by intro ducing a regulariza-
tion parameter

, which controls the magnitude of the co ecients. The Ridge
regression es tim ator is denes as:
^

Ridge
= (
X
T
X
+
I
)

1
X
T
y
The exp ectation and variance of
^

Ridge
are analyzed by substituting
y
=
X 
+

into the estimator and applying the prop erties of exp ectation and vari-
ance op erator. The exp ec tation and variance of Ridge regression estimator are:
E
(
^

Ridge
) = (
X
T
X
+
I
)

1
(
X
T
X
)

V ar
(
^

Ridge
) =
Ë™
2
(
X
T
X
+
I
)

1
X
T
X
(
X
T
X
+
I
)

1
The ee ct of increasing

on the variance is also considered. As

! 1
,
the variance of
^

Ridge
tends to zero, but the bias increases demons trating the
bias-variance trade-o in Ridge regression.
2.7 Bias-Variance Tradeo
Understanding the bias-variance tradeo is crucial in developing mo dels that
generalize well to new data. In the context of regression analysis, particularly
when using p olynomial mo dels, the complexity of the mo del can signicantly
impact b oth the bias and variance of the predictions . Our goal was to ana-
lyze how varying the complexity of our mo dels aects their bias and variance
comp onents.
2.7.1 Mo del Complexity Variation
We varied the de gre e of the p olynomial features in our design matrix, ranging
from 1 to 20. For each degree, we constructed the design m atrix X using the
metho d describ ed earlier and tted the mo dels using Ordinary Le ast Squares
regression.
12
2.7.2 Resampling with Bo otstrap
To estimate the bias and variance comp onents of our mo dels, we employed
the b o otstrap resampling metho d. Bo otstrap allows us to approximate the
sampling distribution of a statistic by resampling w ith replacement from the
original dataset.
Pro cedure
The steps for the b o otstrap metho d are as follows:
1.
Generate Bo otstrap Samples:
For each of b o otstrap iterations, we
will b e using 100 iterations:
(a)
 Sample n data p oints with replacement from the original dataset to
create a b o otstrap sample.
(b)
 Fit the regression mo del on the b o otstrap sample.
(c)
 Predict on the original input data to obtain predictions. These pre-
dictions are used to estimate the bias and variance.
2.
Compute Average Predictions:
Calculate the mean prediction for
each data p oint across all b o otstrap samples.
3.
Compute Overa ll Metrics:
Calculate the mean squared error (MSE),
bias squared, and variance over all data p oints. The exp ected squared
error can b e rewritten as a term which contains the bias and variance of
the mo del itse lf as shown b e low
2.7.3 Bias-Variance Decomp osition Derivati on [4]
The goal is to de rive the decomp osition of the exp ected squared error into
bias, variance, and noise. We want to show that:
E
[(
y

~
y
)
2
] = Bias + Variance +
Ë™
2
where: - Bias =
E
[(
y

E
[ ~
y
])
2
] - Variance =
E
[( ~
y

E
[ ~
y
])
2
] -
Ë™
2
is the
irreducible noise.
Step 1: Expand the s quared term Starting with the exp ected squared
error:
E
[(
y

~
y
)
2
]
We expand the squared term:
E
[(
y

~
y
)
2
] =
E
[
y
2

2
y
~
y
+ ~
y
2
]
This can b e split into three terms:
13
E
[
y
2
]

2
E
[
y
~
y
] +
E
[ ~
y
2
]
Step 2: Add and subtract
E
[ ~
y
]
We intro duce the exp ecte d value of ~
y
into the expression:
E
[(
y

~
y
)
2
] =
E
[(
y

E
[ ~
y
] +
E
[ ~
y
]

~
y
)
2
]
Expanding this gives:
E
[(
y

E
[ ~
y
])
2
+ 2(
y

E
[ ~
y
])( ~
y

E
[ ~
y
]) + ( ~
y

E
[ ~
y
])
2
]
Step 3: Simplify the cross term
The cross te rm
E
[2(
y

E
[ ~
y
])( ~
y

E
[ ~
y
])] b ecomes zero b ecause
E
[ ~
y
] is
a constant and indep endent of
y
:
E
[2(
y

E
[ ~
y
])( ~
y

E
[ ~
y
])] = 0
Thus, we are left with:
E
[(
y

~
y
)
2
] =
E
[(
y

E
[ ~
y
])
2
] +
E
[( ~
y

E
[ ~
y
])
2
]
Step 4: Add the irreducible noise term
The term
E
[(
y

E
[ ~
y
])
2
] represents the bias s quared, and
E
[( ~
y

E
[ ~
y
])
2
]
represents the variance. Additionally, the re is a n irreducible noise term,
Ë™
2
, which accounts for the variance in the observations around the true
values.
Thus, we obtain the nal bias-variance decomp osition:
E
[(
y

~
y
)
2
] = Bias
2
+ Variance +
Ë™
2
where: - Bias =
E
[(
y

E
[ ~
y
])
2
] - Variance =
E
[( ~
y

E
[ ~
y
])
2
] -
Ë™
2
is the
irreducible error (nois e).
2.7.4 Implementation Details
We implemented the b o otstrap bias-variance estimation using Python and
the following libraries:
â€¹
NumPy
for numerical computations.
â€¹
scikit-learn
for regression mo dels and resampling utilities. [5]
14
The core implementation is as follows:
1
2
# Create an empty array to store bootstrap predictions
3
z_pred
 =
 np
.
empty((z_test
.
shape[
0
], n_bootstraps))
4
5
for
 i
 in
 range
(n_bootstraps):
6
# Resample the training data
7
X_, z_
 =
 resample(X_train, z_train)
8
z_pred[:, i]
 =
 model
.
fit(X_,
z_)
.
predict(X_test)
.
ravel()
,
!
9
10
# Reshape z_test for broadcasting
11
z_test_reshaped
 =
 z_test[:, np
.
newaxis]
# Shape
(n_samples, 1)
,
!
12
13
# Calculate bias, variance, and error
14
error[degree]
 =
 np
.
mean((z_test_reshaped
 -
 z_pred)
 **
 2
)
15
mean_z_pred
 =
 np
.
mean(z_pred, axis
=
1
)
# Shape
(n_samples,)
,
!
16
bias[degree]
 =
 np
.
mean((z_test
 -
 mean_z_pred)
 **
 2
)
17
variance[degree]
 =
 np
.
mean(np
.
var(z_pred, axis
=
1
))
2.8 Cross-validation resampling (f )
Cross-validation resampling is a technique used to enhance the ge ne ral-
ization of a mo del while minimizing the risk of overtting. This me tho d
involves rep eatedly splitting the dataset into training and testing subsets,
allowing for a more robust evaluation of the m o del's p erformance. The
parameter
k
sp ecies the numb er of folds in the cross-validation pro cess,
determining how many distinct iterations of the dataset are generated.
In each ite ration, the dataset is divided into
k
subsets, or folds . One fold is
reserved for testing, while the remaining
k

1 folds are utilize d for training
the mo del. This pro cess is rep eated
k
times, ensuring that each fold serves
as the test set exactly once. By employing this approach, c ros s-validation
creates a series of varied test-train splits, resulting in a comprehensive as-
sessment of the mo del's ability to generalize across dierent data segments.
Cross-validation was inte grate d into the pro ject using [5] Scikit-learn's
cr ossv al scor e
along with the MSE scorer. This implementation was ap-
plied in conjunction with Scikit-learn's Ordinary Leas t Squares (OLS),
Ridge, and Lasso re gression mo dels. As the custom re gre ssion algorithms
15
were not co ded to accommo date cross-validation, this integration allowed
for the generation of MSE scores for each regression metho d. The mean of
these sc ore s was the n utilized for visualization, providing insights into the
p erformance and accuracy of the dierent regression techniques. Below is
the co de for implementation.
Co de: [5] Cross validation implementation using scikit
This piece of co de will run the k-fold cross validation 5 tim es, one time
for each
k
2
[5
;
10], resulting in an output of MSE-scores for OLS, Ridge
and Lasso regression for each
k
. This was done in order to analyse how
higher values of
k
aect the MSE-score, as well as e ns uring the mo dels
were not overtted. The visualization b elow shows a typical run of the
cross-validation co de.
Figure 2: Cross-validation: MSE for dierent values of k
16
2.9 Analysis of real data
2.9.1 Data Prepro cessi ng
The real-world terrain data from Norway, provided in
.tif
format, was
loaded using the
imread
function from the
imageio
library and visualized
as a 2D grayscale image. To prepare the data for regression, we atte ned
the 2D grid co ordinates into 1D arrays and intro duced noise to simulate
real-world conditions:
Co de: Preparing the data
2.9.2 Polynomial Design Matrix
We applied p olynomial regression by constructing a
design matrix
that
includes p olynomial te rm s up to degree 5. This design matrix allows the
mo del to t more complex surfaces to the terrain data. The following
function constructs the design matrix fo r our 2D grid:
Co de: Design matrix
2.9.3 Data Standardization and Train-Test Split
Since Ridge and Lasso are sensitive to the scale of input data, we stan-
dardized the design matrix so that all features have a mean of 0 and a
standard deviation of 1. We also split the data into training (70% ) and
testing (30%) sets:
17
Co de: Train-test split
2.9.4 Mo del Implementat ion
We implemented three regress ion mo dels:
Ordinary Least Squares (OLS)
,
Ridge
, and
Lasso
. These mo dels were tted to the standardized training
data, and the ir p erformance was evaluated based on the
Mean Squared
Error (MSE)
on the test set [3]. The core implementation is as follows:
Co de: Implementations
18
2.9.5 Mo del Eval ua tion: MSE Calculation
After tting the mo dels, we used the test se t to evaluate their p erformance
by calculating the MSE. Lower MSE values indicate b etter p erformance,
as they measure how close ly the predicted te rrain heights match the actual
data.
Co de: MSE c alc ulation
2.9.6 Cross-Validation with K-Fold
To ensure the mo dels generalized well across dierent parts of the data, we
used
5-Fold Cross-Validation
. This technique splits the training data into
5 subsets, ts the mo del on 4 subsets, and validates it on the remaining
subset. This pro cess is rep eated 5 times , with each fold serving as the
validation set once. The average MSE across the 5 folds was computed to
assess mo del stability.
Co de: Cross-validation
19
2.9.7 Visualization of Results
To b etter interpret the re sults, we visualized b oth the original terrain and
the terrain predicted by the OLS mo del. This was done using 3D s urface
plots, which allowed us to compare the complexity of the original terrain
to the mo del's predicted surface.
Co de: Visualization
In summary, we implemented p olynomial regress ion mo dels (OLS, Ridge,
and Lasso) and evaluated their p erformance using MSE and cross-validation.
The visualization conrmed that while OLS and Ridge p erformed simi-
larly, b oth failed to capture the detailed complexity of the terrain, and
Lasso p erformed worse due to oversimplication. Cross-validation pro-
vided further conrmation that Ridge did not signicantly outp erform
OLS.
3 Results
Github: https://github.com/Theo dorJaarvik/Fys -stk3155
Pro ject- 1
3.1 Ordinary Least Squares Regression Results
3.1.1 Mo del Performance Metrics
The table b elow visually des cirb es how the MSE and R2 score changes as
mo del complexity increases.
20
Figure 3: MSE and R2 score change as p olynomial degree increases
This summarizes the MSE and R2 scores for the OLS mo del at dierent
p olynomial degrees ranging from 1 to 5. These metrics provide insights
into how well the mo del ts the training data and how it ge ne raliz es to
unseen data.
As observed, the training MSE decrease s with increasing p olynomial de-
gree, indicating that the mo del ts the training data more closely. The
training R2 score also increases, which signies a b etter t. However,
the test MSE do es not decrease at the same rate, and the test R2 score
shows diminishing returns b eyond a p olynomial degree of 4. This suggests
that while the mo del is improving its t on the training data, its ability
to generalize to unseen data do e s not improve prop ortionally, hinting at
p otential overtting at higher deg re es.
3.1.2 Beta Co ecients and Polynomial Degree
In our analysis we also plotted the b e ta co ecients as a function of mo del
complexity. The following plot visualizes how the co ecients change with
higher p olynomial degrees.
21
Figure 4: Co ecient change with p olynomial degree increase
The b eta co ecients in Ordinary Least Squares regression evolve signif-
icantly as the degree of the p olynomial increases, as shown ab ove. For
lower degrees (0 to 2), the co ecients remain small and close to zero,
indicating a stable mo del with limited complexity.
Howe ver, b eyond degree 2, the co ecients for higher-order terms diverge,
with some increas ing sharply, while others show strong negative trends.
This reects the mo del's increasing exibility as it adapts to the data, often
at the cost of overtting, as indicated by large c o ecient magnitudes.
The growing volatility in

values with higher p olynomial degrees high-
lights the trade-o b etween bias and varianc e, where reduced bias leads
to increased sensitivity to data noise . Controlling this eect is es sential to
avoid overtting, esp ecially when dealing with complex mo dels.
3.1.3 Comparison with Scikit-Learn Implementation
To validate the correctness of our manual OLS implementation, we com-
pared the results with thos e obtained using the OLS mo del from the scikit-
learn library [5]. The gure b e low illustrates the comparison of the MSE
values from b oth implementations across dierent p olynomial degrees.
22
Figure 5: Comparison with [5] scikit learn implementation
The identical MSE values b etween our manual mo del and a scikit learn
mo del conrm that our OLS implementation is consistent with the stan-
dard library function, thus validating the accuracy of our co de.
3.1.4 Visualizing the Mo del Fit
To assess how well the OLS mo del captures the underlying structure of the
Franke Function, we visualized the predicted surface alongside the true
datap oints. The gure b elow shows the data p oint plots of the Franke
Function and the OLS-predicted values us ing a fth-degree p olynomial.
23
Figure 6: OLS-mo de l c ompared to Franke-function, fth-degree p olyno-
mial
The OLS m o del with a fth-degre e p olynomial captures the main features
of the Franke Function, including the p eaks and valleys. Notice that this
visualization is of a Franke function that has no added noise to it. With a
p erfectly distributed Franke function, we still notice s ome discrepancies,
indicating that while the mo del p erform s well, there is ro om for improve-
ment in capturing ner details.
3.1.5 Impact of noise
In our main OLS imple mentation we tted a mo de l to the Franke Func-
tion with added noise that is normally distributed. As we can tell form
the graph that describ ed the MSE and R2 scores as a function of mo del
complexity we observe high values for MSE and R2 relative to the numer-
ical intervals our indep endent variables op erate in. We analysed this by
reducing the gaussuian noise added to the Franke Function by a factor
of ten (0.1). The graph b elow descirb es the same re la tion with mo del
complexity but with signicantly lower values of MSE.
Figure 7: MSE and R2 score with less noise
24
3.1.6 Eect of data amount and distribution
As we can observe in the graph ab ove, the tes t and training values are
almost identical when we reduce the noise. This implies that our mo del is
tting extremely well on testing data. This can b e explained by the fac t
that the original indep endent variable s x and y are evenly distributed in
a grid using the following co de:
1
# Define the fixed step size and generate grid points
2
step_size
 =
 0.02
# Adjust this value for different step
sizes
,
!
3
x_values
 =
 np
.
arange(
0
,
 1
, step_size)
4
y_values
 =
 np
.
arange(
0
,
 1
, step_size)
5
x_grid, y_grid
 =
 np
.
meshgrid(x_values, y_values)
6
x
 =
 x_grid
.
ravel()
7
y
 =
 y_grid
.
ravel()
When our data is distributed eve nly and we have a large amount of it,
our mo del c aptures the underlying features of the true data very well and
this is reected by the fact that there is a minuscule dierence b etween
the testing and training error values.
3.1.7 Eect of Feature Scaling
Feature scaling proved to b e a crucial step in our analysis. Without scal-
ing, the OLS mo del would exp erience numerical instability due to the
large dierences in the magnitudes of the p olynomial features, esp ecially
at higher degrees. By applying standardization, we ensured that all fea-
tures contributed equally to the mo del, which im proved the numerical
stability and convergence of the mo del tting pro cess.
3.1.8 Overtting and Mo del Complexity
As we increase d the complexity of the mo del, we observed that the training
and test MSE continued to decrease. If we were to intro duce higher com-
plexity, and or fewe r data p oints, less evenly distributed data and more
Gaussian noise , we would observe a divergence b etween the training and
testing MSE values. The widening gap would indicate that the mo del is
overtting to the training data including its noise, which adverse ly aects
its generalization to new data.
25
Our ndings highlight the imp ortance of selecting an appropriate p oly-
nomial degree to balance the trade-o b etween bias and variance. The
validation against scikit-learn's implementation conrms the correctness
of our manual OLS approach.
3.2 Results for Ridge regression
In this section, we present the results obtained from the Ridge regression
analysis applied to the data generated by the Franke Function. As out-
lined in the m etho ds section, we varied the regularization parameter

to
observe its e ect on the mo del's p erformance in terms of Mean Squared
Error (MSE) and
R
2
score for b oth the training and test sets.
3.3 Eect of Regularization on Mo del Performance
To inves tigate the eec t of regularization, we tested multiple values of

in the range from 0.01 to 100. The resulting training and tes t errors, as
measured by MSE and R2, are displaye d in the graph b elow. The results
highlight the tradeo b etwee n bias and variance in Ridge regression as

increases.
Figure 8: MSE and R2 for training and test sets as a function of lamb da
3.3.1 Mean Squared Error
From Figure 8, we observe that the training MSE increases as

increases.
This is exp ected, as the increasing regularization term shrinks the magni-
tude of the regression co ecients, reducing the mo del's ability to t the
26
training data closely. For smaller values of

, the training error is low ,
indicating a go o d t to the training data. However, as

increases, the
mo del b ecomes less exible and underts the data, leading to higher train-
ing errors. For testing data we observe a similar trend but with a lower
rate of change.
3.3.2 R2 Score
Figure 8 shows the R2 score as a function of

. For the training data, the
R2 sc ore decreases monotonically with increasing

, consistent with the
increase in training error. At sm all values of

, the mo del achieves a high
R2 score on the training data, but this comes at the cost of overtting, as
seen by the lower R2 score on the te st data.
For the test data, the R2 score shows a similar trend. The R2 score
consistently decline, indicating that the mo del b egins to undert the test
data due to excessive regularization.
3.3.3 Optimal Value of

The optimal regularization parameter

was found to b e the smallest

in our interval. This supp oses that we the range in which we tested

values where to big. To o large

values generalizes the mo del to o much
and negatively impacts b oth the training and testing MSE.
3.4 Lasso regression
To analyze the eectiveness of the Lasso regression metho d, it is imp or-
tant to observe how the Mean Squared Error (MSE) and
R
2
scores change
across dierent values of lamb da.
27
Figure 9: Lasso Regression: MSE and R2 s cores for dierent values of

Figure 9 illustrates the relationship b etween thes e metrics and lamb da,
where increasing lamb da values lead to a rise in MSE and a decrease in
R
2
. As lamb da increases, Lasso p enalizes larger co ecients more strongly,
ultimately shrinking ma ny of them toward zero, like seen in gure 11:
Figure 10: Co ecient shrinking for dierent values of lamb da
The crossover p oint b etween the MSE and
R
2
curves highlights the impact
of regularization strength on mo del p erformance. Ide ntifying an optimal
lamb da is essential to balance undertting and overtting, which cannot
28
b e determined analytically but must b e evaluated e mpirically, as shown in
gure 10. Lo oking at it, one can see that an optimal lamb da value would
lie in the range of (10

2
;
10

1
), as it is in this range you get a mo del that
is not overtted, while still explaining the variance of the data.
3.4.1 Comparing Lasso with Ridge and OLS Regression
Figure 11: Visualiz ation of the dierent prediction mo dels as compared
to the original franke function : tuned lasso
Figure 11 visualizes how dierent prediction mo dels approximate data
generated by the Franke function. It is evident that the Lasso mo del
struggles to capture the de taile d features of the data, even when ne-
tuned (

=0.01). Instead of capturing the nuanced variations, Lass o only
grasps the broad strokes of variance in the z-values. This is likely a con-
sequence of its L1 regularization, which, as discussed earlier, pushes some
co ecients toward zero. In the context of top ographical data analysis,
where many features can b e c rucial, this le vel of simplication may lead
to the loss of imp ortant characteristics, making Lasso less suitable for this
typ e of analysis.
29
Ridge regression, on the other hand, p erforms slightly b etter by captur-
ing some of the p e aks and valleys of the Franke function. However, the
mo del still pro duces a highly generalized surface, smo othing over many of
the ner details and reducing the variance b etween extreme values. Like
Lasso, Ridge also app ears undertted for this problem, lacking the nec-
essary complexity to accurately predict the full range of z-values. This
suggests that b oth Lasso and Ridge, while useful for avoiding overtting,
may not b e well-suited for mo deling real-world top ographical data where
capturing subtle fe atures is imp ortant.
Ordinary Least Squares (OLS) regress ion app ears to oer the b est p erfor-
mance among the three mo dels. As seen in gure 11, its surface follows
the variations in the Franke function data more closely than e ithe r Lasso
or Ridge. However, similar to the regularized mo dels, OLS still fails to
capture every ne detail of the original function. While it provides a b et-
ter t, OLS is not immune to the limitations of linear mo dels, esp ecially
when approximating highly non-linear surfaces like the Franke func tion.
3.5 Statistical interpretation of LR
The primary fo cus was on deriving the exp ectation values and the variance
for the estimated co ecient under Ordinary Least Squares (OLS) and the
regression metho ds.
(a)
Exp ectation of
y
i
:
It was shown that the exp ectation of
y
i
, given
by
y
i
=
X
i

+

i
, is:
E
(
y
i
) =
X
i

This conrms that the exp ected value of each observation is deter-
mined by the linear mo del.
(b)
Variance of
y
i
:
The variance of
y
i
was derived as:
Var(
y
i
) =
Ë™
2
This implies that all observations have the same variance, corresp ond-
ing to the noise in the mo del, which is
Ë™
2
.
(c)
Exp ectation of
^

:
The OLS estimate
^

was shown to b e an unbi-
ased estimator:
E
(
^

) =

This means the OLS estimates are exp ected to b e centered around
the true parameter values.
(d)
Variance of
^

:
The variance of
^

was calculated as:
Var(
^

) =
Ë™
2
(
X
T
X
)

1
This provides the disp ersion of the estimated co ecients around the
true values.
30
(a)
Exp ectation of
^

Ridge
:
The exp ectation of the Ridge regression
estimator was derived as:
E
(
^

Ridge
) = (
X
T
X
+
I
)

1
X
T
X 
Unlike OLS, Ridge regression intro duces bias into the estimates, as
the exp ected value is not exactly

for
 >
0.
(b)
Variance of
^

Ridge
:
The variance of the Ridge regression co ecients
was found to b e:
Var(
^

Ridge
) =
Ë™
2
(
X
T
X
+
I
)

1
X
T
X
(
X
T
X
+
I
)

1
The variance decreases as

increases, indic ating that the estimate s
b ecome more stable but at the cost of increased bias.
(c)
Eect of

on Variance:
As

! 1
, the variance of
^

Ridge
ap-
proaches zero:
Var(
^

Ridge
)
!
0 as

! 1
This demonstrates the bias-variance tradeo inherent in Ridge regres-
sion, where larger regularization parameters stabilize the estimates
but intro duce m ore bias.
The exercises highlighted the trade-os b etwe en bias and variance in OLS
and Ridge regression. While OLS provides unbiased estimate s, Ridge
regression reduces variance at the cost of intro ducing bias, demonstrating
a fundamental asp ect of regularization in statistical mo deling.
31
3.6 Bias variance tradeo us ing b o otstrap
We analyzed the Ordinary Least Squares (OLS) using
LinearRegression()
:
3.6.1 Visualization and Analysis
Figure 12: MSE, Bias and variance for dierent p olynomial degrees
After computing the bias, varianc e, and MSE for each p olynom ial degre e,
we plotted these quantities against the mo del complexity. This visualiza-
tion help ed us observe the tradeo b etween bias and varianc e:
â€¹
Bias:
Generally decreases with increasing mo del complexity, as the
mo del b ecomes more exible and can b etter t the true data.
â€¹
Variance:
Generally increases with increasing mo del complexity, as
the mo del b ecomes more sensitive to uctuations in the training data.
â€¹
MSE:
The sum of bias squared and variance, indicating the total
exp ected prediction error.
3.6.2 Interpreting the Bias-Variance Tradeo
By analyzing the plots, we identied the p olynomial degree at which the
MSE is minimized, representing the optimal balance b etween bias and
variance. This de gree indicates the mo del complexity that achieves the
32
b est generalization p erformance on unseen data.
Key Observat ions
(a)
Low-Degree Polynomials:
High bias due to undertting; the mo del
is to o sim ple to capture the underlying structure.
(b)
High-Degree Polynomi als:
High variance due to overtting; the
mo del captures noise in the training data.
Through this bias-variance analysis, we systematically investigated how
mo del complexity and regularization te chniques aect the p erformance of
regression mo dels on the Franke Function. This metho dology provides a
framework for se le cting mo dels that achieve an optimal trade o, e nhancing
their predictive capabilities on new data.
3.6.3 Impact of Numb er of D ata Points
The bias-variance tradeo is not only inuenced by mo del complexity
but als o by the am ount of available data. As we increase the numb er of
data p oints, the variance of the mo del te nds to decrease, since the mo del
b ecomes less sensitive to uctuations in any single subset of the data. On
the other hand, bias rem ains large ly unaec ted by the numb er of data
p oints, as it is primarily de p endent on the mo del's complexity.
Key Observat ions:
(a)
More Data Poi nts:
As the numb e r of data p oints increases, the
mo del tends to exhibit lower variance b ecause it b ecomes more stable
across dierent samples. This c an mitigate the overtting problem,
esp ecially in high-degree p olynomial mo dels.
(b)
Fewer Data Points:
With fewer da ta p oints, the mo del is more
prone to variance, as it can overt the limited data, e sp ecially when
the mo del com ple xity is high.
(c)
Bias:
Increasing the numb er of data p oints do es not signicantly
aect bias, as the bias is determined by how well the mo del struc ture
captures the true underlying relationship, which is a function of the
mo del complexity rather than the data size.
Thus, increas ing the data p oints typically helps to reduce the variance
without impacting bias, resulting in im proved generalization. This insight
emphasizes the imp ortance of having sucient data when training complex
mo dels to control the variance while maintaining a low bias.
33
3.7 k-fold cross validation
The results of applying K-fold cross-validation to dierent regression mo d-
els showed that increasing the numb er of folds did not signicantly aect
the MSE score. Validating the previous results and showing that the
mo dels are not overtted. This implies that the mo dels are generalized
, explaining the absent variance in the resulting MSE s cores. Figure 13
illustrates how little the MSE varies with dierent values of k:
Figure 113: Cross-validation: MSE for dierent values of k, Lasso : not
tuned
According to the results, Ordinary Least Squares (OLS) and Ridge re-
gression mo dels p erformed well on this dataset, while the Lasso regression
exhibited p o or p erformance. One p otential reason for Lasso's underp erfor-
mance could b e exce ssive p enalization of imp ortant fe atures , which reduces
the accuracy of the mo de l. Applying the principles learned in the lasso
regression analysis, one can tune the

parameter to b etter the mo del.
Setting the

value to 0.01, instead of the default 0.1, you get:
34
Figure 14: Cross-validation: MSE for dierent values of k, Lasso : tuned
While tuning the

parameter improved Lasso's p erformance, b oth OLS
and Ridge regression still outp erformed the Lasso mo del. This suggests
that for this particular dataset, Lasso's strength in feature selection may
not b e as b enecial, and the regularization it imp oses might b e to o restric-
tive, even after tuning. Further tuning or adjusting the data prepro ces sing
steps (e.g., scaling or feature transformation) could p otentially enhance
Lasso's p erformance, though Ridge and OLS app ear to b e b etter suited
to this dataset overall, as they are shown to not b e ove r tted.
3.7.1 Comparing Bo otstrap and k-fold Cross- validation
Bo otstrap
Bo otstrap re sampling provides a direct estimate of mo del
variance by generating multiple training sets from the original sample,
making it eective for small datasets or when a single train-test split may
not b e representative. It ca ptures mo del variability well but can over-
represent certain data p oints due to sampling with replacement, p oten-
tially inating variance estimates.
K-fold cross validation
splits the dataset into k subsets, ensuring each
data p oint serve s as a test se t once. It is more computationally e cient
than b o otstrap, particularly for larger datasets, and avoids the redun-
dancy of resampling. Our analysis showed that increasing the numb er of
folds had little eect on MSE, conrming the generalization and robust-
ness of OLS and Ridge mo dels.
35
Overall, K-fold pro duced more stable and slightly lower MSEs than b o ot-
strap. Both metho ds armed that OLS and Ridge outp erformed Lasso,
with Lasso showing higher variance and sensitivity to regularization tun-
ing.
4 Discussion
In this study, p olynomial regress ion mo dels (OLS, Ridge, and Lasso) were
applied to predict terrain data from Norway. While the metho ds c ap-
tured broad trends in the data, there were notable limitations and areas
where the mo dels underp erformed. This section critically ass esses the
strengths and weaknesses of the approach, placing the results in the con-
text of machine learning metho ds for re gre ssion, and oers insights for
future improvements.
4.1 Mo del Performance and Analysis
The primary evaluation metric used to asses s mo del p erformance was the
Mean Squared Error (MSE). The results for OLS, Ridge, and Lasso were
relatively consistent, with OLS and Ridge showing near-identical MSE val-
ues ( 29,444), while Lasso exhibited a signicantly higher MSE ( 31,734).
These values suggest several key p oints ab out the mo dels' ability to cap-
ture the underlying terrain data:
OLS and Ridge Regularization: The minimal dierence in MSE b etwee n
OLS and Ridge indicates that Ridge's regularization had little to no ef-
fect in this sp ecic context. Regularization is generally applied to prevent
overtting, but in this case, the data and design matrix did not intro duce
enough variance or noise for regularization to s ignicantly improve the
p erformance. This suggests that OLS was already well-suited to the prob-
lem at hand, and the data was not overtted, meaning that the addition
of a p enalty term for Ridge was not necessary.
Lasso's Po or Performance: Lass o, on the other hand, underp erformed
with a higher MSE value, despite its ability to shrink co ecients. This
was likely due to La sso's over-regularization, which caused it to zero out
co ecients that may have b een imp ortant in capturing the ner details
of the terrain. The sparse nature of Las so regression makes it well-suited
for datasets with many irrelevant features, but for this dataset where the
features are generated from a p olynomial design matrix Lasso's simpli-
cation caused the mo del to lose valuable information ab out the terrain's
complexity.
36
These results align with the literature on the limitations of Lasso in
highly structured datasets, such as those pro duced by p olynomial features.
Ridge, which retains all co ecients but p enalizes large values, p erforms
b etter in such contexts, although in this case, the gains were marginal due
to the relatively low complexity of the dataset.
4.2 Visual Results and Interpretation
The visualizations of the terrain predictions provided additional ins ights
into how well the mo dels captured the underlying patterns in the data.
While the OLS and Ridge mo dels were able to approximate the general
shap e of the terrain, b oth mo dels pro duced overly smo oth surfaces that
lacked the sharp p eaks and valleys characte ris tic of the real-world terrain
data. The OLS-predicted surface, for example, smo othe d out ne r de tails ,
indicating that the p olynomial degree of 5 might not have b een sucient
to capture the high-frequency variations present in the data.
Figure 15: OLS Prediction (Degree 5)
The lack of ne details in the OLS and Ridge predictions highlights a
key lim itation of using low-degree p olynomial regression. Although the
37
MSE values indicated decent p erformance, the mo dels failed to represe nt
the detailed variations in the terrain. This undertting problem could
p otentially b e addressed by increasing the p olynomial degree, but doing
so comes with the risk of overtting, esp ecially if the mo del b ecomes to o
complex relative to the size of the training data.
Figure 16: Original Terrain
The visual comparison b etween the original terrain and the OLS-predicted
terrain clearly s hows that the predicted surface is much smo other and
misses the detailed p eaks and valleys observed in the original data. [1]
4.3 Eectiveness of Cross-Validation
The use of 5-Fold C ros s-Validation conrmed that the OLS and Ridge
mo dels generalized well across dierent data splits, with consistent MSE
values across the folds. The slight dierence b e tween the cross-validated
MSE values for OLS and Ridge (29473 vs 29474) reinforces the earlier
conclusion that Ridge's regularization provided minimal b enets. This
38
suggests that the variance in the data was low enough that OLS was able
to generalize eectively without requiring any regularization.
On the other hand, Lasso's cross-validated MSE ( 31,760) was considerably
higher than OLS and Ridge, furthe r validating its tendency to oversimplify
the mo del by eliminating to o m any co ecients. This b ehavior of Lasso
aligns with known issues in terrain mo deling and other datasets where
many fe ature s are relevant and rem oving them leads to undertting.
4.4 Limitations and Future Directions
Several limitations b ecame apparent during this study, providing avenues
for future im provements:
â€¹
Polynomial Degree
: The p olynomial degree chose n (5) may have
b een to o low to capture the intricacies of the terrain. A higher p oly-
nomial degree could b etter mo del the sharp variations in the data,
though it risks overtting. Future work could explore cross- validation
or informa tion criteria (e.g., AIC, B IC) to select the optimal p olyno-
mial degree, balancing bias and variance.
â€¹
Mo del Selection
: While this study was limited to OLS, Ridge, and
Lasso, more advanced mo dels such as decision trees, random forests,
or neural ne tworks may provide sup erior p erformance in terrain mo d-
eling tasks. These mo dels are b etter suited for capturing complex,
nonlinear relationships and do not require fe ature e nginee ring via
p olynomial expansions. Neural networks, for example, could p oten-
tially learn intricate patterns from the terrain data without relying
on predened p olynomial terms.
â€¹
Feature Scaling and Tuning
: Although the data was standard-
ized for Ridge and Lasso, more careful tuning of the regularization
parameters (e.g., alpha) could lead to b etter results. A comprehen-
sive grid search or random search over hyp erparameters could further
improve Ridge and Lasso's p erformance, esp ecially if the terrain data
presents subtle nonlinear relationships.
â€¹
Data Resolution
: The resolution of the terrain data itself could in-
uence mo del p erformance. Finer resolution data could lead to b ette r
predictions but would als o require more complex mo dels capable of
handling such data. In this study, the p olynomial degree may not
have b e en sucient to fully exploit the available data's resolution.
4.5 Context and Relevance of the Work
This work ts into the broader context of regression analysis applied to
geographical and terrain data. Polynomial regres sion is a common ap-
proach for tting s urface data due to its simplicity and interpretability.
39
Howe ver, the ndings of this study highlight the limitations of classical
regression metho ds in mo deling complex, real-world terrain data. Sp eci-
cally, terrain data often contain high-frequency variations that are dicult
to capture with low-degree p olynomials or simple regularized linear m o d-
els.
In practical applications, more sophis tic ated machine learning algorithms
such as neural networks or ensemble metho ds might b e more appropriate
for high-resolution terrain data. These mo dels can automatically capture
complex nonlinearities without requiring explicit p olynomial feature e n-
gineering. Nonetheless, this study demonstrates the value of regression
mo dels as a baseline approach, providing insights into how regularization
aects p erformance in the context of smo oth surfaces like terrain.
In conclusion, the analys is showe d that while OLS and R idge regression
provided reasonable approximations of the terrain, b oth mo dels were lim-
ited in capturing ner details. Lasso, due to its aggressive regularization,
underp erformed, indicating that terrain mo deling requires pres erving co-
ecients that contribute to the complexity of the surfac e. The vis ualiza-
tions underscored the need for higher-order p olynomials or more advanced
mo dels to capture the full range of variations prese nt in the terrain. Cross-
validation further validated the robustness of OLS and Ridge, but more
sophisticated mo dels may b e required to fully capture the terrain's com-
plexity.
5 Conclusion
The ob jective of this study was to analyze and compare the p erformance
of three regression metho ds |Ordinary Least Squares (OLS), Ridg e, and
Lasso|on b oth synthetic data generated by the Franke Function and real-
world terrain data. Using metrics such as Mean Squared Error (MSE)
and
R
2
scores, along with resampling techniques like Bo otstrap and K-
fold cross-validation, we evaluated each mo del's ability to generalize and
capture underlying patterns in the data.
Our main ndings indicate that OLS and Ridge regre ssion eectively mo d-
eled datasets of mo derate complexity. OLS served as a solid baseline, pro-
viding go o d ts to the data but showing susceptibility to overtting, es-
p ecially with higher-degree p olynomials and noisy data. Ridge regression,
with its
L
2 regularization, mitigated overtting by shrinking co ecient
magnitudes, resulting in b etter generalization on b oth synthetic and real-
world datasets. Conversely, Lasso regression underp erformed due to its
aggressive
L
1 regularization, which excessively p enalized imp ortant fea-
tures and led to undertting|particularly evident in the complex terrain
40
data where it failed to capture crucial details.
Interpreting the se results, we observe that while regularization is essen-
tial for controlling overtting, the choice b etween
L
1 and
L
2 p enalties
signicantly impacts mo del p erformance. Ridge's ability to retain all fea-
tures while reducing their impact proved advantageous for datasets where
every feature contributes meaningfully. Lasso's feature selection c apabili-
ties are b enecial in high-dimensional s pac es with many irrelevant features
but can b e detrimental when imp ortant features are p enalized to o harshly.
Lo oking ahead, several avenues for future work pres ent themselves . One
p otential improvement is to explore higher-degree p olynom ial mo dels or
interaction terms to b etter capture the nonlinearities inherent in c omple x
datasets like terrain data.
Discussing the pros and cons of the metho ds, OLS is straightforward and
interpretable but prone to overtting complex data. Ridge regression of-
fers a balance by reducing overtting without eliminating features, making
it suitable for datasets with multicollinearity. Lasso regression excels in
feature selection but may undert when all features are relevant, as seen
in our terrain analysis.
In conclusion, selecting a regression metho d that aligns with the complex-
ity and characteristics of the data is crucial. While OLS and Ridge showed
comp etence in handling mo derate com plexities , their limitations|esp ecially
in capturing the ner de ta ils of real-world terrain|highlight the need
for more sophisticated mo dels. Future work should fo cus on integrating
more exible, non-linear metho ds and p e rforming thorough hyp erparam-
eter tuning to enhanc e mo del p erformance in complex, real-world appli-
cations.
References
[1]
 Christopher M Bishop. Pattern recognition and machine learning,
2006.
[2]
 Morten Hjorth-Jensen. Applied data analysis and machine learning,
2021.
[3]
 Gareth James, Daniela Witten, Trevor Hastie, and Rob ert Tibshirani.
An intro duction to statistical learning with applications in r, 2013.
[4]
 Tibshirani R. Frie dm an J. MHastie, T. The elements of statistical
learning: Data mining, inference, and prediction. springer series in
statistics., 1009.
41
[5]
 Fabian Pedregosa, Gael Varo quaux, Alexandre Gramfort, Vincent
Michel, Bertrand Thirion, Vincent Dub ourg, Jake Vanderplas, Ana
Passos, Rudolf We is s, and Tom Kee p ers. Sc ikit-le arn: Machine learn-
ing in python, 2011.
42
